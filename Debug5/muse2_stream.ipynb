{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e05bd337",
   "metadata": {},
   "source": [
    "# README: About this script\n",
    "\n",
    "It is optimal to run this notebook in a separate python environment. I highly recommend running this using `virtualenv` or an **Anaconda Jupyter Notebook**. This script will not explain how to set up these environments - it's good practice to learn how to set up these environments on your local machines.\n",
    "\n",
    "All relevant files and APKs are available at two locations: one on Google Drive, and another on Github. In either, you are free to download sample EEG data and the `.apk` file for the VR application. \n",
    "\n",
    "* **Google Drive Public Folder**: [https://drive.google.com/drive/folders/1hBshxxs70bkTOT0cswSCXQ0fp2N0_dlN?usp=sharing](https://drive.google.com/drive/folders/1hBshxxs70bkTOT0cswSCXQ0fp2N0_dlN?usp=sharing)\n",
    "* **Github Public Repository**: [https://github.com/SimpleDevs-AR-VR/CSGY9223-ARVR-EEGSS](https://github.com/SimpleDevs-AR-VR/CSGY9223-ARVR-EEGSS)\n",
    "\n",
    "## About this Notebook\n",
    "\n",
    "This script was run on an Anaconda Jupyter Notebook environment. The script contains the following segments:\n",
    "\n",
    "1. [**Package Imports**](#1.-Package-Imports): these are key important packages necessary to run this script. **Make sure to run all these before proceeding!**\n",
    "    1. [_PIP Installations_](#PIP-Installations)\n",
    "    2. [_Package Imports + Helper Functions_](#Package-Imports-+-Helper-Functions)\n",
    "2. [**User Defined Hyperparameters**](#2.-User-Defined-Parameters): parameters that users must type in to run the script. Defaults are provided. **Make sure to run all these before proceeding!**\n",
    "3. [**EEG Stream**](#3.-EEG-Stream-Pipeline): these code segments are necessary to connect to an LSL Stream that connects the python script to the Muse 2 Headband, as well as aggregate EEG data pythonically.\n",
    "    1. [_Collecting Data from VR_](#Collecting-Data-From-VR)\n",
    "        1. [_Installing \"ARVR-EEGSS.apk\"_](#Installing-\"ARVR-EEGSS.apk\")\n",
    "        2. [_Extracting Data After a Session_](#Extracting-Data-After-a-Session)\n",
    "    2. [_About Threads_](#About-Threads)\n",
    "    3. [_Raw EEG Streaming and Queueing_](#Raw-EEG-Streaming-and-Queueing)\n",
    "    4. [_Epoch Handling and Power Spectral Density Calculation_](#Epoch-Handling-and-Power-Spectral-Density-Calculation)\n",
    "    5. [_Wrapping Everything + Displaying Results_](#Wrapping-Everything-+-Displaying-Results)\n",
    "        1. [_Experiment Setup_](#Experiment-Setup)\n",
    "        2. [_Connect to an LSL Stream_](#Connect-to-an-LSL-Stream)\n",
    "        3. [_Running the Stream_](#Running-the-Stream)\n",
    "4. [**EEG Processing**](#4.-EEG-Processing): these code segments are necessary to handle EEG raw data post-processing and generation of video to coincide with recordings of the VR simulation from participants.\n",
    "    1. [_Handling VR Events_](#Handling-VR-Events)\n",
    "    2. [_EEG to PSD Conversion_](#EEG-to-PSD-Conversion)\n",
    "        1. [_Class Parameters_](#Class-Parameters)\n",
    "        2. [_Plotting Epoched PSD Over Time_](#Plotting-Epoched-PSD-Over-Time)\n",
    "        3. [_Plotting PSD Videos_](#Plotting-PSD-Videos)\n",
    "    3. [_Handling Participants' Data_](#Handling-Participants'-Data)\n",
    "        1. [_Participant #1_](#Participant-#1)\n",
    "        2. [_Participant #2_](#Participant-#2)\n",
    "        3. [_Participant #3_](#Participant-#3)\n",
    "        4. [_Participant #4_](#Participant-#4)\n",
    "        5. [_Participant #4_](#Participant-#5)\n",
    "\n",
    "Take care to run only the code portions relevant to your intended goal. If you want to stream data from an LSL stream, use the **EEG Stream** code; if you want to process EEG data, use the code in **EEG Procesing**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4566b037",
   "metadata": {},
   "source": [
    "## External Hardware/Software\n",
    "\n",
    "If you wish to run this script on your own with your own hardware setup, you must have the following:\n",
    "\n",
    "* **Muse 2 Headband**: The primary Brain-Computer Interface (BCI) used during this study. Available here: [https://choosemuse.com/products/muse-2](https://choosemuse.com/products/muse-2)\n",
    "* **Petal Metrics**: The LSL pipe application that directly connects to the Muse 2 headband via bluetooth and creats an LSL stream that can be accessed by external applications and scripts. Available here: [https://petal.tech/downloads](https://petal.tech/downloads)\n",
    "* **Meta Quest Pro**: The primary head-mounted display (HMD) VR headset. While it is not necessary to use one, the project was designed with this particular HMD headset in mind.\n",
    "\n",
    "If you want further info on how to properly collect data from the VR headset, please refer to **EEG Stream**. If you don't have access to BCI data or any of the hardware, provided is a `sample.zip` file that contains dummy data. The same dummy data is also available on [Google Drive](https://drive.google.com/file/d/1MrMkgPo894oH4kjxDW2QTkfSCh6VR81k/view?usp=drive_link) and on [Github](https://github.com/SimpleDevs-AR-VR/CSGY9223-ARVR-EEGSS)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7705dd6b",
   "metadata": {},
   "source": [
    "# 1. Package Imports\n",
    "\n",
    "**Make sure to run all these before proceeding!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f13a83",
   "metadata": {},
   "source": [
    "## PIP Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da806d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install pylsl\n",
    "!pip install pygatt\n",
    "!pip install mne\n",
    "!pip install pandas\n",
    "!pip install pyserial\n",
    "!pip install esptool\n",
    "!pip install pyautogui\n",
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e2078c",
   "metadata": {},
   "source": [
    "## Package Imports + Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b2ea6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" ---------------- \"\"\"\n",
    "\"\"\"    The Basics    \"\"\" \n",
    "\"\"\" ---------------- \"\"\"\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "import threading\n",
    "from pylsl import StreamInlet, resolve_stream, resolve_byprop\n",
    "from queue import Queue\n",
    "from IPython.display import display, clear_output\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from scipy.signal import butter, lfilter, find_peaks\n",
    "\n",
    "import pyautogui\n",
    "import mne\n",
    "\n",
    "from importlib import reload\n",
    "import conditions\n",
    "\n",
    "\n",
    "\"\"\" ---------------------- \"\"\"\n",
    "\"\"\"    HELPER FUNCTIONS    \"\"\"\n",
    "\"\"\" ---------------------- \"\"\"\n",
    "\n",
    "def flatten(L):\n",
    "  for item in L:\n",
    "    try:\n",
    "      yield from flatten(item)\n",
    "    except TypeError:\n",
    "      yield item\n",
    "\n",
    "# Src of Function: https://stackoverflow.com/questions/24005221/ipython-notebook-early-exit-from-cell\n",
    "class StopExecution(Exception):\n",
    "  def _render_traceback_(self):\n",
    "    return []\n",
    "\n",
    "def InchToPixel(inches):\n",
    "  return int(inches * 96)\n",
    "\n",
    "# These three functions sort filenames \"humanly\" (as opposed to the default lexicographical sorting that computers understand)\n",
    "# The user-friendly function to use is `sort_nicely(l)`, where `l` is a list of files where all contents are of type ____<#>.png\n",
    "# Source: https://nedbatchelder.com/blog/200712/human_sorting.html\n",
    "def tryint(s):\n",
    "    try:\n",
    "        return int(s)\n",
    "    except:\n",
    "        return s\n",
    "def alphanum_key(s):\n",
    "    \"\"\" Turn a string into a list of string and number chunks. \"z23a\" -> [\"z\", 23, \"a\"] \"\"\"\n",
    "    return [ tryint(c) for c in re.split('([0-9]+)', s) ]\n",
    "def sort_nicely(l):\n",
    "   return sorted(l, key=alphanum_key) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e3c71a",
   "metadata": {},
   "source": [
    "# 2. User Defined Parameters\n",
    "\n",
    "These are a collection of user-defined hyperparameters that will be used across the application. **Make sure to run all these before proceeding!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e7a298",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" --------------------- \"\"\"\n",
    "\"\"\"    HYPERPARAMETERS    \"\"\"\n",
    "\"\"\" --------------------- \"\"\"\n",
    "\n",
    "_SESSION_NAME = \"EEG Stream\"           # What will you name the session?\n",
    "_EPOCH_SIZE = 2.0                      # What will be the epoch size?\n",
    "_REFERENCE_CHANNELS = [\"TP9\",\"TP10\"]   # Which channels should we re-reference the EEG data to?\n",
    "_FREQUENCY_CHANNELS = [\"AF7\", \"AF8\"]   # Which channels are we interested in observing specifically?\n",
    "_FREQUENCY_RANGE = (0.5, 80)           # What frequency will we bandpass filter for?\n",
    "_POWER_RANGE = (0, 200)                # What is the power range of the Power Spectral Density graph that'll be produced? Purely visual in function\n",
    "_PSD_AVG = \"mean\"                      # When calculating the Power Spectral Density, what metric should we aggregate data samples around?\n",
    "_FIGSIZE = (10, 5)                     # When producing the PSD graph, what will the figure size be? Purely visual in function\n",
    "_CHANNEL_COLORS = [\"blue\", \"red\"]      # When producing the PSD graph, what colors correspond to the channels in _FREQUENCY_CHANNELS? Purely visual in function\n",
    "_SAVE_CSV = True                       # Should we save the raw EEG data as well as the processed data?\n",
    "_VERBOSE = False                       # Should we be verbose and yell at the user about warnings, update messages, print statements, etc?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" ========================================================================\n",
    "THE FOLLOWING ARE NOT HYPERPARAMETERS AND MUST NOT BE ALTERED IN ANY WAY\n",
    "======================================================================== \"\"\"\n",
    "\n",
    "dirpath = f\"logs/{_SESSION_NAME}\"\n",
    "dirpath_query = dirpath+\"/\"\n",
    "dirpath_num = 1\n",
    "while os.path.exists(dirpath_query):\n",
    "    dirpath_num += 1\n",
    "    dirpath_query = dirpath + f\"_{dirpath_num}/\"\n",
    "dirpath = dirpath_query\n",
    "os.makedirs(dirpath)\n",
    "filename_processed = dirpath+\"psd.csv\"\n",
    "filename_raw = dirpath+\"eeg.csv\"\n",
    "filename_settings = dirpath+\"settings.json\"\n",
    "\n",
    "frequency_bands = {\n",
    "  \"delta\": {\"range\":(0.5,4),\"color\":\"darkgray\"},\n",
    "  \"theta\": {\"range\":(4, 8),\"color\":\"lightblue\"},\n",
    "  \"alpha\": {\"range\":(8, 16),\"color\":\"blue\"},\n",
    "  \"beta\":  {\"range\":(16, 32),\"color\":\"orange\"},\n",
    "  \"gamma\": {\"range\":(32, 80),\"color\":\"red\"}   \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48116e64",
   "metadata": {},
   "source": [
    "# 3. EEG Stream Pipeline\n",
    "\n",
    "_**Petal Metrics**, the original LSL Pipe handler that connects the Muse 2 Headband EEG data to an LSL stream, outputs its own EEG files. They're hard to find, though. So if you'd rather handle use Python to handle the EEG data, here's the code to do so._\n",
    "\n",
    "_Initially, I had intended to use multi-threading to handle both EEG raw data saving AND Power Spectral Density (PSD) processing for real-time PSD visualization. However, in practice, this led to too much lag, and the PSD output could not be visually depicted in real-time. Therefore, this section of scipt is dedicated to allowing the user to **debug** if the LSL stream is working as intended. This is necessary because BCIs and EEG data is inherently touch to understand is unintuitive to handle; the debugging method used here allows the user to see if something's amiss with the connectivity of the BCI to the subject's frontal lobe._\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159e37ae",
   "metadata": {},
   "source": [
    "## Collecting Data From VR\n",
    "\n",
    "\n",
    "### Installing \"ARVR-EEGSS.apk\"\n",
    "\n",
    "Included alongside this code is an application called `ARVR-EEGSS.apk` that you can side-load into your Meta Quest Pro. The  method to sideload applications is up to your discretion. A preferred tactic is to use 3rd-party software such as [**SideQuest**](https://sidequestvr.com/) - instructions on how to sideload apps are included [here](https://www.uploadvr.com/sideloading-quest-how-to/). `ARVR-EEGSS.apk` has been provided alongside this code, but you can also access it publicly from [Google Drive](https://drive.google.com/file/d/1aOalvvqPnefyNeQfOK7eCu1SQBCQggAw/view?usp=sharing) and [Github](https://github.com/SimpleDevs-AR-VR/CSGY9223-ARVR-EEGSS).\n",
    "\n",
    "Prior to running the sideloaded application, make sure you calibrate your eye-tracking on your headset, if you care about that kind of thing. \n",
    "\n",
    "Upon successful sideload, you will be able to access the program by following these steps:\n",
    "\n",
    "1. Navigate back to your home menu on your Quest\n",
    "2. Open the \"Applications\" tab.\n",
    "3. There should be a dropdown that lets you sort visible applications. Make sure to select \"Unknown Sources\"\n",
    "4. A new application called \"ARVR-EEGSS\" should be available.\n",
    "\n",
    "Upon first loading the application, you will be prompted whether you want to allow eye tracking and face tracking. It's up to your discretion to enable eye tracking (I highly recommend it), but don't allow Meta to collect your face-tracking data (though that's also up to your perogative too - it's just that the application itself doesn't rely on face tracking).\n",
    "\n",
    "### Extracting Data After a Session\n",
    "\n",
    "When you complete a session and close the app, you can access your positions, orientation, and eye tracking data from your session by connecting your Meta Quest Pro to a computer and navigating to its file system.\n",
    "\n",
    "* **Windows**: Windows systems can automatically enable file system access to the Quest system.\n",
    "* **Mac OSX**: You will need 3rd-party software to access the file system. One recommended application is the \"Android File Transfer\" application, available [here](https://www.android.com/filetransfer/).\n",
    "\n",
    "Upon gaining file system access, you should follow this pathing to access the generated `.csv` file:\n",
    "\n",
    "> 1. /root/\n",
    ">    1. Android/\n",
    ">        1. data/\n",
    ">            1. com.SimpleDevs.ARVREEGSS/\n",
    ">                1. files/\n",
    "                \n",
    "Inside this folder, you will find `csv` files titled `TestEventWriting-<DATE>.csv`. Not the most intuitive filename, but you will be able to download this file into your local machine easily. Make sure to keep track of which files correspond to which sessions, if you happen to run this application multiple times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1432ad1",
   "metadata": {},
   "source": [
    "## About Threads\n",
    "\n",
    "Because Jupyter Notebooks cannot run two different cells concurrently, we are required to design the program such that the signal acquisituion and processing occurs simultaneously. This can be achieved via \"Threads\" that run concurrently in the same class or function call. They just can't be split into different operations. The functions themselves can be defined in separate cells, but their operation is contingent on one cell call.\n",
    "\n",
    "There will be TWO separate threads:\n",
    "\n",
    "1. **Thread #1** will handle interactions with the LSL stream. This involves pulling the current timestamp and EEG data and storing the data into predesignated epoch sizes. The epochs will be cached in _Queue #1_ that will be communicated with our second thread.\n",
    "2. **Thread #2** handles epochs directly and will take on the brunt of the Power Spectral Density calculation for each epoch. Each PSD calculated will have its data pushed to _Queue #2_, of which our 3rd thread will handle displaying the PSD itself.\n",
    "\n",
    "What's somewhat... weird about threads is that you can use global variables for reading parameters or such. However, if you need to update variables at some point in a thread, you can't just update the global variables directly. This is because if global variables are shared at any point, then there might be a moment where they will be altered by two threads simultaneously. To prevent this, we perform some fixes:\n",
    "\n",
    "* We use queues to communicate data between threads. They also act as a FIFO list where epochs are processed in order\n",
    "* We also initialize a \"settings\" JSON dictionary that we'll use to print out info about our stream into. This will also be shared alongside the queues.\n",
    "* If in the event one or more threads needs to be shut down, we'll use _events_ to communicate between threads and shut down their while loops if needed.\n",
    "\n",
    "Outside the two primary threads, we will be using a final `while` loop to display the PSD at the current timestamp. This might get a bit dicey with how finnicky Matplotlib can be with dynamic graphs, and generating the graphs themselves may take time, so this is placed outside of the two threads to prevent throttling. Furthermore, `matplotlib` doesn't like being used in threads, hence why we're putting this outside of either thread."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03938c6",
   "metadata": {},
   "source": [
    "## Raw EEG Streaming and Queueing\n",
    "\n",
    "This code runs the 1st thread logic. This method reads the EEG data from the LSL stream, saves the data, and condenses rows into epochs. Each epoch produced is pushed to an output queue; this output queue is the input queue of the 2nd thread, which handles epochs directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2cc2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PipeEEGStream(out_queue, filename, event, s):\n",
    "  \n",
    "  # We set up the epoch as an array, where each row is an entry from our inlet stream\n",
    "  current_epoch = []\n",
    "  \n",
    "  # Each epoch is defined by their ID (integer), start time (UNIX Seconds), and end time (UNIX Seconds)\n",
    "  # We'll also initialize a `row_id` (integer) that we'll use as a unique counter for each row\n",
    "  epoch_id = None\n",
    "  epoch_start = None\n",
    "  epoch_end = None\n",
    "  row_id = 0\n",
    "  \n",
    "  # Prepare a CSV writer to record this data, and add a heading as the first entry\n",
    "  csvfile = open(filename, 'w', newline='\\n')\n",
    "  writer = csv.writer(csvfile, delimiter=',', quotechar='\"')\n",
    "  writer.writerow([\"id\",\"lsl_ts\",\"unix_ts\",\"TP9\",\"AF7\",\"AF8\",\"TP10\",\"AUX\"])\n",
    "  \n",
    "  # We'll now continuously iterate via a while loop that only ends if the `event` (defined outside this function) is set elsewhere (maybe during a keyboard exception, for example)\n",
    "  while not event.is_set():\n",
    "    \n",
    "    # We'll use a try-catch to ensure we capture data only as long as the stream is open\n",
    "    try:\n",
    "      \n",
    "      # Update our row ID\n",
    "      row_id += 1\n",
    "      \n",
    "      # Receive the current data from the stream, Extract the int of the timestamp\n",
    "      sample, timestamp = inlet.pull_sample()\n",
    "      \n",
    "      # We need to derive the unix timestamp of this pull. We can't trust the timestamp given by the inlet, which provides the LSL's timestamp\n",
    "      unix_timestamp = time.mktime(datetime.now().timetuple())\n",
    "      \n",
    "      # We'll first write the sample directly into our CSV before anything else\n",
    "      # We'll ensure that we flatten all the data into a single row first\n",
    "      writer.writerow(flatten([row_id, timestamp, unix_timestamp, sample]))\n",
    "      \n",
    "      # Now we'll prepare our payload for our row. Note that this is a multi-dimensional row insofar as python is concerned.\n",
    "      timestep_int = int(unix_timestamp)\n",
    "      row = [row_id, timestamp, unix_timestamp, sample]\n",
    "      \n",
    "      # We need to decide what to do with the row. The issue is if the row fits outside an epoch, we have to initialize a new epoch\n",
    "      # For the first epoch, which doesn't exist yet, we'll create it with the first row. All subsequent rows will have to be handled somehow\n",
    "      if epoch_id is None:\n",
    "        \n",
    "        # We initialize our epoch as this is the first data in the stream\n",
    "        epoch_id = 1\n",
    "        epoch_start = unix_timestamp\n",
    "        epoch_end = unix_timestamp + _EPOCH_SIZE\n",
    "        \n",
    "        # Add the first row as part of our first epoch\n",
    "        current_epoch.append(row)\n",
    "        \n",
    "        # We actually update our settings data, which we passed as an argument, here. This is because\n",
    "        # Threads cannot UPDATE values outside the thread, so we have to pipe the settings JSON here\n",
    "        # Might as well update the stream start settings here - makes sense, because this is our first epoch and first stream row\n",
    "        s[\"stream_start\"] = unix_timestamp\n",
    "        \n",
    "      elif unix_timestamp >= epoch_start and unix_timestamp < epoch_end:\n",
    "        # In this case, we're stil in the same epoch, we're fine. We just add the row to our current epoch\n",
    "        current_epoch.append(row)\n",
    "      \n",
    "      else:\n",
    "        # In this case, we now know the current row doesn't fit in our current epoch. So we have to start a new epoch\n",
    "        \n",
    "        # We'll pass our current epoch to our `out_queue`\n",
    "        out_queue.put({\n",
    "          \"epoch_id\": epoch_id, \n",
    "          \"epoch_start\":epoch_start, \n",
    "          \"epoch_end\":epoch_end, \n",
    "          \"data\": current_epoch\n",
    "            })\n",
    "                \n",
    "        # We now have to reset everything\n",
    "        epoch_id += 1\n",
    "        epoch_start = epoch_end\n",
    "        epoch_end = epoch_start + _EPOCH_SIZE\n",
    "        current_epoch = []\n",
    "      \n",
    "        # Append to the new epoch\n",
    "        current_epoch.append(row)\n",
    "    \n",
    "    except IndexError:\n",
    "      # In this case, there might have been an error in the stream. We have to close early\n",
    "      print('[ERROR] Stream has been closed')\n",
    "      # We set the event, to let other streams know that they've gotta end\n",
    "      event.set()\n",
    "      break\n",
    "\n",
    "  # At the end of it all, we need to update our settings JSON to let the user (later on) know when the stream ended and the overall stream duration\n",
    "  s[\"stream_end\"] = unix_timestamp\n",
    "  s[\"stream_duration\"] = s[\"stream_end\"] - s[\"stream_start\"]\n",
    "  \n",
    "  # Finally, let out a shout letting us know that the stream handling is over.\n",
    "  if _VERBOSE: print(\"ENDING STREAM HANDLING\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb57466",
   "metadata": {},
   "source": [
    "## Epoch Handling and Power Spectral Density Calculation\n",
    "\n",
    "The code blocks below handle the logic of the 2nd thread to process epochs of data. There are two major functions used: \n",
    "\n",
    "* `GetPSD()` looks at each epoch's data and computes the PSD of that data.\n",
    "* `ProcessEpochs()` handles the queue from the 1st thread, calls `GetPSD()` for each epoch that comes through, and passes the output to the 3rd thread in a different output queue.\n",
    "\n",
    "Here, a special EEG-handling python package called `mne` is used to calculate the PSD and perform filtering operations on each epoch. When handling EEG data specifically, there's some important practices that are usually followed:\n",
    "\n",
    "1. **Re-Referencing**: When EEG data is measured generally, most BCIs rely on a \"reference\" channel where signal data is \"based\" off of (some BCIs don't have a reference electrode and are thus called \"reference-free\"). Re-referencing is thus re-calculating each electrode signal so that their signal strength is oriented around a new reference channel. This practice is often used to emphasize the signal strength of particular electrodes without losing any information about the original signal strengths. For example, if we are interested in Channels 1 and 2 and our pre-designated reference channel is Channel 3 but Channel 3 is physically close to Channels 1 and 2, then the signal strength of both channels will be drastically weakened. By re-referencing the data to a different channel - perhaps a Channel 4 that's on the other side of the head, then the signal strengths of Channels 1 and 2 will be strengthened.\n",
    "\n",
    "2. **Frequency Filtering**: We are generally interested in only a certain range of frequencies in our EEG data. Frequencies that are too high might be indicative of noise rather than meaningful data. Therefore, we have to filter the raw signal so that only a certain range of frequencies is processed. This is typically done using _bandpass_, _high-pass_, _low-pass_, _notch_, or _butterworth_ filters.\n",
    "\n",
    "3. **Power Spectral Density**: the raw EEG data received from the LSL stream is in the time-domain (signal strength over time). We want to observe how the signal behaves across different frequencies, however. We can actually do so, by transforming the signal from the time domain to the frequency domain using techniques such as the _Fast Fourier Transform_ or _Welch's Method_. Welch's method is particularly interesting because its method does more than just transform the X-axis from time to frequency; it handles data so that the Y-axis is not just the signal amplitude, but rather the signal's \"power\" (dB) and elevates the signal strength to purely a positive-value range, thus simplifying the analysis further.\n",
    "\n",
    "In our analysis, the Muse 2 Headband uses 5 electrodes: `TP9`, `TP10` (close to the Temporal Lobe), `A7`, `A8` (close to the Frontal Lobe), and an `AUX` channel that is only present in some Muse headbands; the Muse 2 headband defaults its reference channel to an additional electrode that's close to `A7` and `A8`. We will be performing a bandpass filter by restricting the frequency between 0.5 to 80 Hz and re-referencing the signals to the average of the `TP9` and `TP10` channels (this is called a \"bipolar linked-mastoid\" referencing). The PSD is calculated via Welch's Method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a082b2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetPSD(epoch):\n",
    "    \n",
    "  # Firstly, get some stats about our data\n",
    "  # This is ultimately to determine the frequency of the stream data, given this epoch\n",
    "  n = float(len(epoch[\"data\"]))\n",
    "  duration = epoch[\"epoch_end\"] - epoch[\"epoch_start\"]\n",
    "  freq = round(n / duration)\n",
    "  if _VERBOSE: print(n, duration, freq)\n",
    "    \n",
    "  # We'll use the data above to define our mne info\n",
    "  muse2_info = mne.create_info([\"TP9\",\"AF7\", \"AF8\", \"TP10\"], freq, ch_types='eeg', verbose=_VERBOSE)\n",
    "\n",
    "  # Secondly, we need to transpose `epoch[\"data\"]` so that each row represents each channel\n",
    "  data = np.transpose([el[3][:-1] for el in epoch[\"data\"]])\n",
    "  _mne = mne.io.RawArray(data, muse2_info, first_samp=0, copy='auto', verbose=_VERBOSE)\n",
    "\n",
    "  # Fourthly, we will re-refernece them if provided the necessary argument\n",
    "  if _REFERENCE_CHANNELS is not None:\n",
    "    _mne.set_eeg_reference(ref_channels=_REFERENCE_CHANNELS, verbose=_VERBOSE)\n",
    "  \n",
    "  # Thirdly, If we need to filter, we do so here\n",
    "  if _FREQUENCY_RANGE is not None:\n",
    "    try:\n",
    "      _mne.filter(l_freq=_FREQUENCY_RANGE[0], h_freq=_FREQUENCY_RANGE[1], verbose=_VERBOSE)\n",
    "    except:\n",
    "      if _VERBOSE: print(\"filter not applied\")\n",
    "  # Fifthly, calculate the PSD and the respective powers and frequencies\n",
    "  _psd = _mne.compute_psd(average=_PSD_AVG, verbose=_VERBOSE)\n",
    "  powers, freqs = _psd.get_data(picks=_FREQUENCY_CHANNELS, return_freqs=True)\n",
    "\n",
    "  # return\n",
    "  return powers, freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8719bbbc",
   "metadata": {},
   "source": [
    "This function is the primary function called when the thread is instantiated. It merely takes the queue connected to the 1st reference and, if there is an epoch present, will save the epoch in a 2nd file and send the epoch data to `GetPSD()`. The outputted PSD is sent to a 2nd queue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7043c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProcessEpochs(in_queue, out_queue, filename, event):\n",
    "  \n",
    "  # Prepare a CSV writer to record this data. We're not writing a dictionary file to this, and we won't be adding a header\n",
    "  # The reason for no header is because there are variable # of columns. This is because we'll be saving the output of the PSD calculation to each row.\n",
    "  # The PSD calculation is finnicky because there's no way to guarantee how many frequencies will be popped out of the PSD calculation...\n",
    "  csvfile = open(filename, 'w', newline='\\n')\n",
    "  writer = csv.writer(csvfile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "  \n",
    "  # Our while loop is a bit special, because it ensures that we process all queues first prior to ending the thread\n",
    "  while in_queue.qsize() > 0 or not event.is_set():\n",
    "    \n",
    "    # Naturally, we have to check if our queue size is actually not empty. We continue if it is...\n",
    "    if in_queue.qsize() == 0: continue\n",
    "    epoch = in_queue.get()\n",
    "    \n",
    "    # Calculate the relevant data\n",
    "    powers, frequencies = GetPSD(epoch)\n",
    "    \n",
    "    # to write the row, we need to at least prepend 3 additional columns to the \"powers\" data:\n",
    "    # 1. the epoch id\n",
    "    # 2. the epoch start\n",
    "    # 3. the epoch end\n",
    "    # 4. channel\n",
    "    # We need to do this for each channel\n",
    "    for i in range(len(_FREQUENCY_CHANNELS)):\n",
    "      csv_row = [epoch[\"epoch_id\"], epoch[\"epoch_start\"], epoch[\"epoch_end\"], _FREQUENCY_CHANNELS[i]]\n",
    "      csv_row.extend(powers[i])\n",
    "      writer.writerow(csv_row)\n",
    "    \n",
    "    # We need to output our epoch PSD to our `out_queue` for printing the display!\n",
    "    out_queue.put({\n",
    "      \"id\": epoch[\"epoch_id\"],\n",
    "      \"start\": epoch[\"epoch_start\"],\n",
    "      \"end\": epoch[\"epoch_end\"],\n",
    "      \"freqs\": frequencies,\n",
    "      \"powers\": powers\n",
    "        })\n",
    "\n",
    "  # Finally, if we reach this point, then that means our stream ended. We'll print something to let the user know.\n",
    "  if _VERBOSE: print(\"ENDING EPOCH PROCESSING\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a38b127",
   "metadata": {},
   "source": [
    "## Wrapping Everything + Displaying Results\n",
    "\n",
    "The code blocks below are dedicated to operating the streaming element of this code. It consists of setting up the connection to the LSL stream, calling the two threads, and rending the output of the 2nd thread into a dynamic Matplotlib figure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9c2bd6",
   "metadata": {},
   "source": [
    "### Experiment Setup\n",
    "\n",
    "You want to organize your physical space such that you have a direct line of sight between your hardware running Petal Metrics and this script and the Muse 2 headband. Obstructions such as walls, glass, and partitions will cause the LSL stream to disconnect. An example is shown below of how you may want to organize your equipment.\n",
    "\n",
    "![Google Drive Image](https://drive.google.com/uc?export=view&id=1t0XdezBZz0v-eDf-0myWnSCZKHbfbG2S)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92df4102",
   "metadata": {},
   "source": [
    "### Connect to an LSL Stream\n",
    "\n",
    "Unfortunately, Python is finnicky with LSL streams. The most popular LSL piping package available is `pylsl`, which enables programmers to link directly to LSL streams. However, if an error occurs with the streaming (either there is no stream available or the stream disconnects in the middle of the stream), then the rest of the program will hang, even when called in a thread. To this effect, it's important to ensure that there actually is an existing stream available to look at and secure a way to end the throttling if an error occurs.\n",
    "\n",
    "> The code below MUST be called before running the 2nd code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9d37fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "streams = resolve_byprop('type', 'EEG', timeout=1.0)\n",
    "if len(streams) > 0:\n",
    "  # create a new inlet to read from the stream\n",
    "  inlet = StreamInlet(streams[0])\n",
    "  print(\"LSL stream found. Call data from the inlet via the following command: `sample, timestamp = inlet.pull_sample()`\")\n",
    "else:\n",
    "  inlet = None\n",
    "  print(\"[ERROR] No LSL streams detected. Please turn on an LSL stream through Petal Metrics or other 3rd-party software\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bc3bfa",
   "metadata": {},
   "source": [
    "### Running the Stream\n",
    "\n",
    "The code block below is all that's needed to handle the EEG stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e8b90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to set up our queues\n",
    "epoch_queue = Queue()\n",
    "psd_queue = Queue()\n",
    "\n",
    "# We set up our event to control our while loops\n",
    "stream_ended_event = threading.Event()\n",
    "\n",
    "# We initialize our settings JSON that we'll store at the end of this stream.\n",
    "stream_settings = {\n",
    "  \"name\":_SESSION_NAME,\n",
    "  \"epoch_size\":_EPOCH_SIZE,\n",
    "  \"stream_start\":None,\n",
    "  \"stream_end\":None,\n",
    "  \"stream_duration\":None,\n",
    "}\n",
    "\n",
    "# Final check: is our stream actually a thing?\n",
    "# We'll exit early if it's not and we skipped all the necssary steps beforehand...\n",
    "if inlet is None:\n",
    "  print(\"ERROR: Cannot run threads if our inlet stream is inactive. Please make sure to connect our script to an LSL stream!\")\n",
    "  raise StopExecution\n",
    "\n",
    "# We'll initialize our figure! We're also doing some minor modifications where the plot can work with dynamic rendering on jupyter notebooks\n",
    "hdisplay = display(\"\", display_id=True)\n",
    "fig, ax = plt.subplots(1, 1, figsize=_FIGSIZE)\n",
    "ax.set_xlabel('Frequency (Hz)')\n",
    "ax.set_ylabel('Power (dB)')\n",
    "\n",
    "t1 = threading.Thread(target=PipeEEGStream, args =(epoch_queue, filename_raw, stream_ended_event, stream_settings, ))\n",
    "t2 = threading.Thread(target=ProcessEpochs, args=(epoch_queue, psd_queue, filename_processed, stream_ended_event, ))\n",
    "t1.start()\n",
    "t2.start()\n",
    "\n",
    "# We'll wrap the rest of our function call in a try:except while loop\n",
    "try:  \n",
    "      \n",
    "  # This will run forever until we experience a keyboard interrupt\n",
    "  while 1:\n",
    "    \n",
    "    # Get the current timestamp\n",
    "    unix_timestamp = time.mktime(datetime.now().timetuple())\n",
    "    \n",
    "    # We'll update our display... but only if our queue is not empty\n",
    "    if psd_queue.qsize() > 0:\n",
    "      epoch = psd_queue.get()  \n",
    "      title = f\"EPOCH: {epoch['id']} [-{unix_timestamp - epoch['start']}]\"\n",
    "      # This will clear our data from our figure\n",
    "      ax.cla()\n",
    "      plt.ylim(_POWER_RANGE[0], _POWER_RANGE[1])\n",
    "      plt.xlim(_FREQUENCY_RANGE[0], _FREQUENCY_RANGE[1])\n",
    "      plt.title(title)\n",
    "      # We'll iterate through our channels\n",
    "      for i in range(len(epoch[\"powers\"])):\n",
    "        plt.plot(epoch[\"freqs\"], epoch[\"powers\"][i], label=_FREQUENCY_CHANNELS[i], color=_CHANNEL_COLORS[i])\n",
    "      for key, val in frequency_bands.items():\n",
    "        plt.axvspan(val[\"range\"][0], val[\"range\"][1], color=val[\"color\"], alpha=0.1)\n",
    "      \n",
    "      # We'll update the display\n",
    "      hdisplay.update(fig)\n",
    "    \n",
    "    else:\n",
    "      print(\"No Updates....\")\n",
    "      \n",
    "    # We'll add a time buffer to prevent overload\n",
    "    time.sleep(0.01)\n",
    "    \n",
    "# If at any point we interrupt the script, we have to ensure all threads are closed\n",
    "except KeyboardInterrupt:\n",
    "  print(KeyboardInterrupt)\n",
    "  if _VERBOSE: print(\"Attempting to close threads...\")\n",
    "  \n",
    "  # We tell the threads to end. The `join()` function purely is meant to wait until all threads are guaranteed to be closed.\n",
    "  stream_ended_event.set()\n",
    "  t1.join()\n",
    "  t2.join()\n",
    "  if _VERBOSE: print(\"Threads successfully closed.\")\n",
    "  \n",
    "  # We write the settings as its own JSON file\n",
    "  with open(filename_settings, \"w\") as outfile:\n",
    "    json.dump(stream_settings, outfile, indent=4)\n",
    "  if _VERBOSE: print(\"Settings saved as JSON.\")\n",
    "    \n",
    "# If at any point the script encounters a runtime error, we have to ensure all threads are closed\n",
    "except RuntimeError:\n",
    "  print(RuntimeError)\n",
    "  if _VERBOSE: print(\"Attempting to close threads...\")\n",
    "  \n",
    "  # We tell the threads to end. The `join()` function purely is meant to wait until all threads are guaranteed to be closed.\n",
    "  stream_ended_event.set()\n",
    "  t1.join()\n",
    "  t2.join()\n",
    "  if _VERBOSE: print(\"Threads successfully closed.\")\n",
    "  \n",
    "  # We write the settings as its own JSON file\n",
    "  with open(filename_settings, \"w\") as outfile:\n",
    "    json.dump(stream_settings, outfile, indent=4)\n",
    "  if _VERBOSE: print(\"Settings saved as JSON.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b643363",
   "metadata": {},
   "source": [
    "# 4. EEG Processing\n",
    "\n",
    "Thie rest of the code observes the analysis of EEG data after a participant has had their data collected via human-subjects research. This code is not intended to be run simultaneously to the code above.\n",
    "\n",
    "In essence, we'll be performing a similar PSD calculation to that of the code in `GetPSD()` above. We'll just be handling it so that we can perform the analysis offline. The same logic used to render the dynamic `Matplotlib` figure rendering. The logic is all the same.\n",
    "\n",
    "What's different is that we'll be integrating event data from our VR simulation into the analysis. The VR event data, outputted as a `.csv` file itself, must be generated simultaneously to the EEG signal acquisition and must have events recorded with UNIX timestamps. In our VR simulation, we had the position and orientation of the user recorded, as well as the eye tracking data. A key element of this part of the process is aligning the VR events with the EEG data and making meaningful observations from the convolution of the two data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5743d05",
   "metadata": {},
   "source": [
    "## Handling VR Events\n",
    "\n",
    "As mentioned before, our VR simulation records events that occur in each participant's session. Each event is matched with a UNIX timestamp (ours happens to be in milliseconds, unfortunately) and a description of the event. Depending on how you record the data, some post-procssing is necessary. Therefore, the class below simply handles that with respect to how I managed to record the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da083545",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Events:\n",
    "\n",
    "  def __init__(self, events_filepath, video_filepath=None):\n",
    "    self.raw = pd.read_csv(events_filepath)\n",
    "    self.df = self.raw.copy()\n",
    "\n",
    "    # our data is in Milliseconds UNIX. We need to conver it to Seconds UNIX\n",
    "    self.df[\"unix_msf\"] = pd.to_numeric(self.df[\"unix_ms\"])\n",
    "    self.df[\"unix_ts\"] = self.df[\"unix_msf\"]/1000.0\n",
    "\n",
    "    # We need to divide each event by their trial, at the very least.\n",
    "    self.trials = []\n",
    "    self.positions = []\n",
    "    self.gaze_targets = []\n",
    "    trials = self.df[self.df['title'].str.contains('Trial \\d+ Start', regex= True, na=False)]\n",
    "\n",
    "    # We also need to grab the last unix_ts in the dataframe\n",
    "    self.start = self.df.iloc[0][\"unix_ts\"]\n",
    "    self.end = self.df.iloc[-1][\"unix_ts\"]\n",
    "    self.duration = self.end - self.start\n",
    "\n",
    "    # Iterate through all trials individually\n",
    "    for index, trial in trials.iterrows():\n",
    "      self.trials.append({\n",
    "          \"index\":index,\n",
    "          \"unix_start\":trial[\"unix_ts\"],              # This is the global start time, based on unix\n",
    "          \"rel_start\":trial[\"unix_ts\"] - self.start,  # this is the relative start time, from the start of the recording\n",
    "          \"unix_end\":None,\n",
    "          \"rel_end\":None,\n",
    "          \"duration\":None,\n",
    "          \"positions\":[],\n",
    "          \"gaze_targets\":[]\n",
    "      })\n",
    "\n",
    "    # Now we loop through all our trials\n",
    "    for i in range(len(self.trials)):\n",
    "\n",
    "      # Firstly, we'll capture the player's positions in their own item in the dictionary\n",
    "      upper_ts = self.end\n",
    "      if i < len(self.trials)-1:\n",
    "        upper_ts = self.trials[i+1][\"unix_start\"]\n",
    "\n",
    "      # Second, set the end of the current trial to `upper_ts`\n",
    "      # We also calculate the duration of the trial\n",
    "      self.trials[i][\"unix_end\"] = upper_ts               # Recall: this is the global unix timestamp for the trial end\n",
    "      self.trials[i][\"rel_end\"] = upper_ts - self.start   # Recall: this is the relative unix timestamp from the start unix of the recording\n",
    "      self.trials[i][\"duration\"] = upper_ts - self.trials[i][\"unix_start\"]\n",
    "\n",
    "      # Thirdly, we have to iterate through all our rows! But we need to grab them first\n",
    "      trial_rows = self.df.loc[\n",
    "          (self.df[\"unix_ts\"] >= self.trials[i][\"unix_start\"])\n",
    "          & (self.df[\"unix_ts\"] < upper_ts)]\n",
    "\n",
    "      # We're looking for 1) participant positions, and 2) gaze targets (left eye)\n",
    "      # These will be spans that we'll integrate either on top or below the data plots\n",
    "      # So for each, we'll need some internal consistency\n",
    "      last_position = None\n",
    "      last_gaze_target = None\n",
    "      for index, row in trial_rows.iterrows():\n",
    "\n",
    "        # Is this a position row?\n",
    "        if row[\"event_type\"] == \"Player\" and row[\"title\"] == \"position\":\n",
    "\n",
    "          # Handle the positions\n",
    "          if last_position is None:\n",
    "            last_position = {\"start\":row[\"unix_ts\"],\"end\":row[\"unix_ts\"]+0.1,\"label\":row[\"description\"]}\n",
    "          elif last_position[\"label\"] != row[\"description\"]:\n",
    "            self.trials[i][\"positions\"].append(last_position)\n",
    "            self.positions.append({\n",
    "              \"trial\":i,\n",
    "              \"unix_start\":last_position[\"start\"],\n",
    "              \"rel_start\":last_position[\"start\"] - self.start,\n",
    "              \"trial_start\":last_position[\"start\"] - self.trials[i][\"unix_start\"],\n",
    "              \"unix_end\":last_position[\"end\"],\n",
    "              \"rel_end\":last_position[\"end\"] - self.start,\n",
    "              \"trial_end\":last_position[\"end\"] - self.trials[i][\"unix_start\"],\n",
    "              \"label\":last_position[\"label\"]\n",
    "            })\n",
    "            last_position = {\"start\":row[\"unix_ts\"],\"end\":row[\"unix_ts\"]+0.1,\"label\":row[\"description\"]}\n",
    "          else:\n",
    "            last_position[\"end\"] = row[\"unix_ts\"]\n",
    "\n",
    "          # We also append this row to our positions array as a raw element\n",
    "          #self.positions.append({\n",
    "          #    \"trial\":i,\n",
    "          #    \"unix_ts\":row[\"unix_ts\"],                                   # Recall: this is the global unix ts\n",
    "          #    \"rel_ts\":row[\"unix_ts\"] - self.start,                       # Recall: this is the relative unix ts from the start o the recording\n",
    "          #    \"trial_ts\":row[\"unix_ts\"] - self.trials[i][\"unix_start\"],   # This is the relative timestamp from the start of the trial\n",
    "          #    \"label\":row[\"description\"]\n",
    "          #})\n",
    "\n",
    "        # Is this a global eye tracking row for the left eye?\n",
    "        elif row[\"event_type\"] == \"Global Eye Tracking\" and row[\"title\"] == \"Left\":\n",
    "\n",
    "          # We preprocess the query target to prevent confusion between differen event types\n",
    "          query_target = row[\"description\"]\n",
    "          if query_target == \"SouthSidewalk\": query_target = \"SouthSidewalk_Gaze\"\n",
    "          elif query_target == \"NorthSidewalk\": query_target = \"NorthSidewalk_Gaze\"\n",
    "          elif query_target == \"RoadCrosswalk\": query_target = \"RoadCrosswalk_Gaze\"\n",
    "          elif query_target == \"WalkingSignalCollider\": query_target = \"WalkingSignal\"\n",
    "          elif query_target == \"CarSignalCollider\": query_target = \"CarSignal\"\n",
    "\n",
    "          # process the gaze span\n",
    "          if last_gaze_target is None:\n",
    "            last_gaze_target = {\"start\":row[\"unix_ts\"], \"end\":row[\"unix_ts\"]+0.1,\"label\":query_target}\n",
    "          elif last_gaze_target[\"label\"] != query_target:\n",
    "            self.trials[i][\"gaze_targets\"].append(last_gaze_target)\n",
    "            last_gaze_target = {\"start\":row[\"unix_ts\"], \"end\":row[\"unix_ts\"]+0.1,\"label\":query_target}\n",
    "          else:\n",
    "            last_gaze_target[\"end\"] = row[\"unix_ts\"]\n",
    "\n",
    "          # Add the gaze target as a raw  element\n",
    "          self.gaze_targets.append({\n",
    "              \"trial\":i,\n",
    "              \"unix_ts\":row[\"unix_ts\"],                                   # Recall: this is the global unix ts\n",
    "              \"rel_ts\":row[\"unix_ts\"] - self.start,                       # Recall: this is the relative unix ts from the start o the recording\n",
    "              \"trial_ts\":row[\"unix_ts\"] - self.trials[i][\"unix_start\"],   # This is the relative timestamp from the start of the trial\n",
    "              \"label\":query_target\n",
    "          })\n",
    "\n",
    "      # Lastly, after iterating through all rows, we'll clean up the last ones here\n",
    "      if last_position is not None:\n",
    "        self.trials[i][\"positions\"].append(last_position)\n",
    "        self.positions.append({\n",
    "              \"trial\":i,\n",
    "              \"unix_start\":last_position[\"start\"],\n",
    "              \"rel_start\":last_position[\"start\"] - self.start,\n",
    "              \"trial_start\":last_position[\"start\"] - self.trials[i][\"unix_start\"],\n",
    "              \"unix_end\":last_position[\"end\"],\n",
    "              \"rel_end\":last_position[\"end\"] - self.start,\n",
    "              \"trial_end\":last_position[\"end\"] - self.trials[i][\"unix_start\"],\n",
    "              \"label\":last_position[\"label\"]\n",
    "            })\n",
    "      if last_gaze_target is not None:\n",
    "        self.trials[i][\"gaze_targets\"].append(last_gaze_target)\n",
    "\n",
    "  def GetPositions(self, tmin=None, tmax=None, dt=1.0):\n",
    "    # Since we weren't provided a trial, we'll purely go by tmin and tmax as relative unix timestamps\n",
    "    if tmin is None: tmin = 0\n",
    "    if tmax is None: tmax = self.duration\n",
    "    rows = [row for row in self.positions if row[\"rel_start\"] >= tmin and row[\"rel_start\"]+dt <= tmax]\n",
    "    return rows\n",
    "\n",
    "  def GetGazeTargets(self, trial=None, tmin=None, tmax=None):\n",
    "    if trial is None:\n",
    "      # Since we weren't provided a trial, we'll purely go by tmin and tmax as relative unix timestamps\n",
    "      if tmin is None: tmin = 0\n",
    "      if tmax is None: tmax = self.duration\n",
    "      rows = [row for row in self.gaze_targets if row[\"rel_ts\"] >= tmin and row[\"rel_ts\"] <= tmax]\n",
    "    else:\n",
    "      # since we've been provided a trial #, tmin and tmax are now relative timestamps from the start of the trial\n",
    "      t = trial - 1\n",
    "      if tmin is None: tmin = 0\n",
    "      if tmax is None: tmax = self.trials[t][\"duration\"]\n",
    "      rows = [row for row in self.gaze_targets if row[\"trial\"] == t and row[\"trial_ts\"] >= tmin and row[\"trial_ts\"] <= tmax]\n",
    "    return rows\n",
    "\n",
    "  def GetTrialInfo(self):\n",
    "    for i in range(len(self.trials)):\n",
    "      trial = self.trials[i]\n",
    "      print(f\"Trial #{i+1}:\")\n",
    "      print(f\"\\tCorresponding Index to start: {trial['index']}\")\n",
    "      print(f\"\\tUNIX start: {trial['unix_start']} [Relative start: {trial['rel_start']}]\")\n",
    "      print(f\"\\tUNIX end: {trial['unix_end']} [Relative end: {trial['rel_end']}]\")\n",
    "      print(f\"\\tTotal Duration: {trial['duration']}\")\n",
    "      print(f\"\\tNumber of positions recorded: {len(trial['positions'])}\")\n",
    "      print(f\"\\tNumber of gaze targets: {len(trial['gaze_targets'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22f59bf",
   "metadata": {},
   "source": [
    "## EEG to PSD Conversion\n",
    "\n",
    "This code is essentially similar to that used in the first part of this code, just encapsulated into class form.\n",
    "The tough part to understand is the function `PlotPSDOverTime()`. This function is the primary function used to generate the PSD waveform across time, and has the functionality to save the PSD as a video.\n",
    "\n",
    "### Class Parameters\n",
    "\n",
    "The class initializer parameters are as follows:\n",
    "\n",
    "* `name`: What is the experiment session name?\n",
    "* `filename`: What is the corresponding experiment session's EEG file (must be a `.csv` file generated from the process above)\n",
    "* `timestamp_col`: The original EEG data usually comes with either an `lsl_ts` (the timestamps that correspond to the Muse 2's internal clock) or the `unix_ts` (the global timescale as pertaining to the UNIX timestamp system). \n",
    "* `drop_channels`: either `None` or an array that drops unrelated channels from the original EEG data. If you want to process any channels in your EEG data that feels useless or unecessary, you can do so here prior to anything else.\n",
    "* `relabel`: either `None` or a dictionary that maps the original channel names in your eeg data to discernable channel names.\n",
    "* `min_freq` and `max_freq`: If either are not set to `None`, then the system will attempt a bandpass filter using these frequencies.\n",
    "* `verbose`: Should we be loud when initializing the class?\n",
    "\n",
    "### Plotting Epoched PSD Over Time\n",
    "\n",
    "* `channels`: REQUIRED array of channel names you want to collect PSD data from. These correspond to the remappings you ought to have done when initializing the clas. By default, the channels are `[\"AF7\", \"AF8\"]`.\n",
    "* `start` and `end`: the starting and ending times from the beginning of the session (in seconds) that you want to isolate the PSD analysis within. These are relative timestamps from the beginning of the session - for example, setting `end = 30` means you will end the analysis when 30 seconds has passed from the start of the session.\n",
    "* `dt`:  This is effectively the epoch size (seconds), or what chunk of your original data you want to segment you session. The smaller the `dt`, the more fine-grained your analysis will be, but at the cost of runtime. By default, the epoch size is `1.0` seconds.\n",
    "* `average`: A string value that lets the PSD calculation know how they should handle within-epoch averaging when calculating the PSD. Most people go with the `mean`, but `median` is also available. You can look this up further in the **Python MNE** documentation.\n",
    "* `fmin` and `fmax`: The lower and upper limits to the range of frequencies visualized in the resulting graph. Purely visual, doesn't change the analysis in any way.\n",
    "* `frequencies`: Either `None` or an array of strings that corresponds to the keys in `frequency_bands` wayyy above. These tell the system \"render these frequency bands on the graph. By default, this parameter is set to `[\"alpha\",\"beta\",\"gamma\"]`.\n",
    "* `colors`: These are colors that you want to assign to each of the frequency bans that you designate in `frequencies`. Will cause an error if the number of colors doesn't match the number of frequencies you want to render. By default, the colors are set to `[\"blue\",\"red\",\"orange\"]`.\n",
    "* `title`: If designated, you can decide what to title your figure. If set to `None` or not designated, the title will be auto-generated.\n",
    "* `figsize`: A tuple or 2D list that controls the size of the figure. Purely cosmetic\n",
    "* `peak_threshold`: When detecting peaks, you can control the threshold of power that the peaks are limited to. Any peaks below this threshold are not tracked.\n",
    "* `representative_metric`: When finding which peak to represent a frequency band with, you may have more than one peak in a frequency band. When you have two or more peaks, that means you have a choice of which frequency you want to use as a \"representative\" frequency for that frequency band. You can choose between \"max\" (default) or \"mean\" (makes things make a little more sense).\n",
    "* `events`: If you want to combine VR events with your graph, you can designate those events by passing an **Events** instance that contains your processed events. When done, the graph will visualize the player's position relative to the road and sidewalk.\n",
    "* `save_fig_filename`: If you want to save the figure as a file, you can designate where the file will be saved and what the filename will be. If set to `False` or `None`, a figure will not be saved.\n",
    "\n",
    "\n",
    "### Plotting PSD Videos\n",
    "\n",
    "The function `PlotPSDOverTime()` is a massive function key to this operation. Its parameters are:\n",
    "\n",
    "* `channels`: REQUIRED array of channel names you want to render in the PSD graph. These correspond to the remappings you ought to have done when initializing the class.\n",
    "* `start` and `end`: the starting and ending times from the beginning of the session (in seconds) that you want to isolate the PSD analysis within. These are relative timestamps from the beginning of the session - for example, setting `end = 30` means you will end the analysis when 30 seconds has passed from the start of the session.\n",
    "* `dt`: This is effectively the epoch size (seconds), or what chunk of your original data you want to segment you session. The smaller the `dt`, the more fine-grained your analysis will be, but at the cost of runtime. This also controls the frame rate of your video, if you choose to render a video.\n",
    "* `average`: A string value that lets the PSD calculation know how they should handle within-epoch averaging when calculating the PSD. Most people go with the `mean`, but `median` is also available. You can look this up further in the **Python MNE** documentation.\n",
    "* `pmax`: The upper limit to the power of the PSD (dB). This is purely a visual control that makes sure your figure doesn't go haywire when rendering the figure.\n",
    "* `auto_pmax`: If set to `True`, then the system will attempt to estimate an appropriate `pmax` dynamically as frames play out. This is ignored if `pmax` is set to some power value. If `pmax` is `None`, this is run by default.\n",
    "* `render_dt`: This is purely a visual control over how frequently your figure dynamically changes between renders. This has no effect on the frame rate of the output video. Use this if you are interested in seeing something in your data at a particular epoch.\n",
    "* `frequencies`: Either `None` or an array of strings that corresponds to the keys in `frequency_bands` wayyy above. These tell the system \"render these frequency bands on the graph.\n",
    "* `title`: If designated, you can decide what to title your figure. If set to `None` or not designated, will default to something admittedly more useful.\n",
    "* `figsize`: A tuple or 2D list that controls the size of the figure. Purely cosmetic\n",
    "* `colors`: A list that controls the color of your channels that you designated in `channels`. This MUST match the size of `channels`, otherwise the system will fail.\n",
    "* `detect_peaks`: Should your system detect peaks in your data? If so, will visually highlight what those peaks are.\n",
    "* `peak_threshold`: If you decided to detect peaks, you can control the threshold of power that the peaks are limited to. Any peaks below this threshold are not rendered.\n",
    "* `events`: If you want to combine VR events with your graph, you can designate those events by passing an **Events** instance that contains your processed events.\n",
    "* `save_video_filepath`: If not set to `None`, you must provide a string that tells the system where you want to generate and save the video file to. The video defaults to an `.avi` format and will generate a temporary `temp/` directory that stores each individual frame. Once the video is generated, the system will delete this `temp/` folder automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07c0a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(conditions)\n",
    "\n",
    "class EEGInterpreter:\n",
    "  def __init__(self,\n",
    "      name,\n",
    "      filename,\n",
    "      timestamp_col=\"unix_ts\",\n",
    "      drop_channels=['ch5'],\n",
    "      relabel={'ch1':'TP9', 'ch2':'AF7', 'ch3':'AF8', 'ch4':'TP10'},\n",
    "      min_freq=None,\n",
    "      max_freq=None,\n",
    "      verbose=False\n",
    "               ):\n",
    "    self.name = name\n",
    "    self.raw = pd.read_csv(filename)\n",
    "    self.signal_labels = ['TP9','TP10','AF7', 'AF8']\n",
    "\n",
    "    if drop_channels is not None:\n",
    "      self.raw = self.raw.drop(columns=drop_channels).drop_duplicates()\n",
    "    else:\n",
    "      self.raw = self.raw.drop_duplicates()\n",
    "\n",
    "    if relabel is not None:\n",
    "      self.raw.rename(columns={'ch1':'TP9', 'ch2':'AF7', 'ch3':'AF8', 'ch4':'TP10'}, inplace=True)\n",
    "\n",
    "    # Get some statistics regarding our data\n",
    "    self.unix_start = self.raw.iloc[0][timestamp_col]\n",
    "    self.unix_end = self.raw.iloc[-1][timestamp_col]\n",
    "    self.duration = self.unix_end - self.unix_start\n",
    "    self.size = len(self.raw.index)\n",
    "    self.frequency = round(self.size / self.duration)\n",
    "    \n",
    "    # Copy the data to prevent mutation\n",
    "    self.eeg = self.raw.copy(deep=True)\n",
    "    if verbose: self.PrintEEGHead()\n",
    "  \n",
    "    # Calcualte the muse info, mne, and initialize the psd\n",
    "    muse2_info = mne.create_info([\"TP9\",\"TP10\",\"AF7\", \"AF8\"], self.frequency, ch_types='eeg', verbose=verbose)      \n",
    "    s_array = np.transpose(self.eeg[[\"TP9\", \"TP10\", \"AF7\", \"AF8\"]].to_numpy())\n",
    "    self.mne = mne.io.RawArray(s_array, muse2_info, first_samp=0, copy='auto', verbose=verbose)\n",
    "    self.psd = None\n",
    "\n",
    "    # Filter based on frequency range if provided\n",
    "    if min_freq is not None or max_freq is not None:\n",
    "      self.mne.filter(min_freq, max_freq, verbose=verbose)\n",
    "\n",
    "  def ReReference(self, new_refs=['TP9', 'TP10']):\n",
    "    self.mne.set_eeg_reference(ref_channels=new_refs)\n",
    "\n",
    "  def ComputePSD(self, tmin=None, tmax=None, average=\"mean\"):\n",
    "    self.psd = self.mne.compute_psd(tmin=tmin, tmax=tmax, average=average)\n",
    "\n",
    "  def PrintEEGHead(self, n=10):\n",
    "    display(self.eeg.head(n))\n",
    "  def PlotEEG(self, scaling=\"auto\"):\n",
    "    self.mne.plot(scalings=scaling)\n",
    "\n",
    "  def PlotPSD(self):\n",
    "    if self.psd is None:\n",
    "      print(\"ERROR: PSD not calculated yet!\")\n",
    "      return\n",
    "    self.psd.plot(picks=[\"AF7\",\"AF8\"])\n",
    "\n",
    "  def PlotPSDGantt(self,\n",
    "                  channels=[\"AF7\", \"AF8\"],\n",
    "                  start=None,\n",
    "                  end=None,\n",
    "                  dt=1.0,\n",
    "                  average=\"mean\",\n",
    "                  fmin=None,\n",
    "                  fmax=None,\n",
    "                  frequencies=[\"alpha\",\"beta\",\"gamma\"],\n",
    "                  colors=[\"blue\",\"red\",\"orange\"],\n",
    "                  title=None,\n",
    "                  figsize=(10,4),\n",
    "                  peak_threshold=None,\n",
    "                  representative_metric=\"max\",\n",
    "                  events=None,\n",
    "                  save_fig_filename=False\n",
    "                    ):\n",
    "    \n",
    "    # Reload the conditions to reset colors\n",
    "    reload(conditions)\n",
    "    \n",
    "    # Initialize our figure\n",
    "    fig, ax = plt.subplots(1,1,figsize=figsize)\n",
    "    t = title if title is not None else f\"{self.name} \\n[dt: {dt}]\"\n",
    "    plt.title(t)\n",
    "    if fmin is not None:\n",
    "      plt.ylim(bottom=fmin)\n",
    "    if fmax is not None:\n",
    "      plt.ylim(top=fmax)\n",
    "    \n",
    "    ax.set_xlabel(f\"Time (Seconds, dt = {dt} sec.)\")\n",
    "    ax.set_ylabel(\"Frequency (Hz)\")\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    added_labels = []\n",
    "    \n",
    "    # If we want to save a figure, but it exists, let's delete it\n",
    "    if save_fig_filename is not None and os.path.exists(save_fig_filename):\n",
    "      os.remove(save_fig_filename)\n",
    "      \n",
    "    # Initialize our data arrays\n",
    "    x = []\n",
    "    y = {}\n",
    "    for freq in frequencies:\n",
    "      y[freq] = []\n",
    "    \n",
    "    # Iterate through each epoch\n",
    "    s = start if start is not None else 0\n",
    "    e = end if end is not None else self.duration\n",
    "    for tmin in np.arange(s, e, dt):\n",
    "      \n",
    "      # Add the current timestamp `tmin` to `x`\n",
    "      x.append(tmin)\n",
    "      \n",
    "      # Calculate the timespan, psd from that timespan, and the powers/frequencies in that timespan\n",
    "      tmax = tmin + dt\n",
    "      if tmax > e: break\n",
    "      try:\n",
    "        psd = self.mne.compute_psd(tmin=tmin, tmax=tmax, average=average, verbose=False)\n",
    "        powers, freqs = psd.get_data(picks=channels, return_freqs=True)\n",
    "      \n",
    "        # powers is a multidimensional list; freqs is not\n",
    "        # freqs is technically the x-axis; powers[i] is the data for each channel \"AF7\", \"AF8\", etc.\n",
    "        # We need to calculate the peaks for each channel\n",
    "        peak_freqs = {}\n",
    "        peak_powers = {}\n",
    "        for freq in frequencies:\n",
    "          peak_freqs[freq] = []\n",
    "          peak_powers[freq] = []\n",
    "        \n",
    "        if len(powers) > 0:\n",
    "          for i in range(len(channels)):\n",
    "            # calc peaks for this channel\n",
    "            peaks, _ = find_peaks(powers[i], threshold=peak_threshold)\n",
    "            # `peaks` is a list of indices. \n",
    "            # To get the corresponding frequencies and powers, we need to call `freqs[peaks]` and `powers[i][peaks]`\n",
    "            # We need to separate each peak and determine which frequency band they are in\n",
    "            for peak in peaks:\n",
    "              peak_freq = freqs[peak]\n",
    "              peak_power = powers[i][peak]\n",
    "              for freq in frequencies:\n",
    "                if peak_freq >= frequency_bands[freq][\"range\"][0] and peak_freq < frequency_bands[freq][\"range\"][1]:\n",
    "                  # this peak is inside this frequency band.\n",
    "                  peak_freqs[freq].append(peak_freq)\n",
    "                  peak_powers[freq].append(peak_power)\n",
    "      \n",
    "          # Now, peak_freqs contains the frequencies corresponding to each frequency band\n",
    "          # Similarly, peak_powers contains the powers corresponding to each frequency band\n",
    "          # Now, the last thing to do is, for each frequency band, determine the representative peak\n",
    "          # This is merely the max peak detected\n",
    "          for freq in frequencies:\n",
    "            if len(peak_powers[freq]) > 0:\n",
    "              if representative_metric == \"mean\":\n",
    "                y[freq].append(np.mean(peak_freqs[freq]))\n",
    "              else:\n",
    "                max_peak_index = np.argmax(peak_powers[freq])\n",
    "                y[freq].append(peak_freqs[freq][max_peak_index])\n",
    "            else:\n",
    "              y[freq].append(0)\n",
    "      \n",
    "          # once we end the `tmin` loop, we should have:\n",
    "          # `x`: the time series\n",
    "          # `y`: a dictionary for each frequency- each frequency has the same length as `x`\n",
    "          # We just need to plot\n",
    "          for i in range(len(frequencies)):\n",
    "            freq = frequencies[i]\n",
    "            col = colors[i]\n",
    "            plt.plot(x, y[freq], label=freq, color=col)\n",
    "            if freq in added_labels: continue\n",
    "            event_line = Line2D([0], [0], label=freq, color=col)\n",
    "            handles.extend([event_line])\n",
    "            added_labels.append(freq)\n",
    "            \n",
    "      except:\n",
    "        print(\"No EEG data detected\")\n",
    "        break\n",
    "      \n",
    "    # OPTIONAL: if provide events, we can begin to give more details\n",
    "    \n",
    "    if events is not None:\n",
    "      # For now, we'll just print the position data\n",
    "      positions = events.GetPositions(tmin=s, tmax=e, dt=dt)\n",
    "      # Each position is, thankfully, already tagged with `rel_start` and `rel_end` and \"label\"\n",
    "      # which tells us the relative start and ends based on the beginning of the VR session\n",
    "      # All we need to do is plot axvspans between these ranges\n",
    "      for pos in positions:\n",
    "        col = conditions.label_color_map[pos[\"label\"]]\n",
    "        plt.axvspan(pos[\"rel_start\"], pos[\"rel_end\"], color=col.color, alpha=col.alpha)\n",
    "        if pos[\"label\"] in added_labels: continue\n",
    "        event_line = Line2D([0], [0], label=pos[\"label\"], color=col.color)\n",
    "        handles.extend([event_line])\n",
    "        added_labels.append(pos[\"label\"])\n",
    "\n",
    "    # Add a legend\n",
    "    plt.legend(handles=handles)\n",
    "    \n",
    "    # If indicated, save the fig\n",
    "    if save_fig_filename is not None:\n",
    "      plt.savefig(save_fig_filename, bbox_inches=\"tight\")\n",
    "    \n",
    "    # Show the plt\n",
    "    plt.show()\n",
    "\n",
    "  def PlotPSDOverTime(self,\n",
    "                      channels=[\"AF7\",\"AF8\"],\n",
    "                      start=None,\n",
    "                      end=None,\n",
    "                      dt=1.0,\n",
    "                      average=\"mean\",\n",
    "                      pmax=None,\n",
    "                      auto_pmax=True,\n",
    "                      render_dt=0.1,\n",
    "                      frequencies=None,\n",
    "                      title=None,\n",
    "                      figsize=(10,4),\n",
    "                      colors=[\"blue\",\"red\"],\n",
    "                      detect_peaks=False,\n",
    "                      peak_threshold=None,\n",
    "                      events=None,\n",
    "                      save_video_filepath=None,\n",
    "                      delete_video_frames=True\n",
    "                        ):\n",
    "    \n",
    "    # Reload the conditions to reset colors\n",
    "    reload(conditions)\n",
    "    \n",
    "    # We need to grab the first and last timestamp of our data, as well as our duration\n",
    "    first_timestamp = self.raw.iloc[0][\"lsl_ts\"]\n",
    "    last_timestamp = self.raw.iloc[-1][\"lsl_ts\"]\n",
    "    total_time = last_timestamp - first_timestamp\n",
    "\n",
    "    # Initialize the figure\n",
    "    #t = title if title is not None else self.name\n",
    "    hdisplay = display(\"\", display_id=True)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "    ax.set_xlabel('Frequency (Hz)')\n",
    "    \n",
    "    # IF we're saving a video, then we'll need to initialize an array to store the generated images\n",
    "    # We'll e using `render_dt` as the interval for each frame in the video\n",
    "    if save_video_filepath is not None:\n",
    "      video_dirname = os.path.dirname(save_video_filepath)+'/temp/'\n",
    "      # We go on a bit of a file-killer spree. If the directory already exists, we have to terminate it early. Then we can create our own temp folder\n",
    "      if os.path.exists(video_dirname):\n",
    "        shutil.rmtree(video_dirname)\n",
    "      os.makedirs(video_dirname)\n",
    "      # If a video with the same name happens to exist, we must delete it first\n",
    "      if os.path.exists(save_video_filepath):\n",
    "        os.remove(save_video_filepath)\n",
    "\n",
    "    # Iterate through each epoch\n",
    "    s = start if start is not None else 0\n",
    "    e = end if end is not None else total_time\n",
    "    fn = 0\n",
    "    for tmin in np.arange(s, e, dt):\n",
    "      \n",
    "      # Increment frame # count\n",
    "      fn += 1\n",
    "      \n",
    "      # Calculate the timespan, psd from that timespan, and the powers/frequencies in that timespan\n",
    "      tmax = tmin + dt\n",
    "      if tmax > e: break\n",
    "      try:\n",
    "        psd = self.mne.compute_psd(tmin=tmin, tmax=tmax, average=average, verbose=False)\n",
    "        powers, freqs = psd.get_data(picks=channels, return_freqs=True)\n",
    "      \n",
    "        # Clear the graph, and set the title and y-limit\n",
    "        ax.cla()\n",
    "        t = title if title is not None else f\"Epoch {tmin}\\n[@ {self.unix_start + tmin}]\"\n",
    "        plt.title(t)\n",
    "        if pmax is not None:\n",
    "          plt.ylim(top=pmax)\n",
    "\n",
    "        # Begin plotting\n",
    "        if len(powers) > 0:\n",
    "          for i in range(len(channels)):\n",
    "            plt.plot(freqs, powers[i], label=channels[i], color=colors[i])\n",
    "            if detect_peaks:\n",
    "              peaks, _ = find_peaks(powers[i], threshold=peak_threshold)\n",
    "              plt.plot(freqs[peaks], powers[i][peaks], \"x\")\n",
    "          if frequencies is not None:\n",
    "            for f in frequencies:\n",
    "              plt.axvspan(frequency_bands[f][\"range\"][0], frequency_bands[f][\"range\"][1], color=frequency_bands[f][\"color\"], alpha=0.1)\n",
    "\n",
    "        # If events are provided, we'll parse the events based on tmin and tmax, which are relative unix timestamps to start of the recording\n",
    "        # All eye targets are added to the legend of the plot, allowing us to see what the eye targets were at that time interval\n",
    "        handles, labels = plt.gca().get_legend_handles_labels()\n",
    "        if events is not None:\n",
    "          # query events\n",
    "          added = []\n",
    "          for row in events.GetGazeTargets(tmin=tmin, tmax=tmax):\n",
    "            if row[\"label\"] not in added:\n",
    "              handles.append(Line2D([], [], marker='.', color='white', linestyle='None', label=row[\"label\"]))\n",
    "              added.append(row[\"label\"])\n",
    "        plt.legend(handles=handles)\n",
    "\n",
    "        # If we're saving a video, we have to save the figure frame to a temporary directory we created above. \n",
    "        # We'll concatenate the video frames later\n",
    "        if save_video_filepath is not None:\n",
    "          plt.savefig(video_dirname+\"file%02d.png\" % fn, bbox_inches=\"tight\")\n",
    "\n",
    "        # Finally, we display the figure\n",
    "        hdisplay.update(fig)\n",
    "\n",
    "        # Update the max\n",
    "        power_min, power_max = plt.ylim()\n",
    "        if auto_pmax and (pmax is None or power_max > pmax):\n",
    "          pmax=power_max\n",
    "\n",
    "        # Sleeeeep\n",
    "        time.sleep(render_dt)\n",
    "    \n",
    "      except:\n",
    "        print(\"No EEG Data found. Ending plotting.\")\n",
    "        break\n",
    "\n",
    "    # If we are saving the video, we should have all the frames added to our video's temporary directory\n",
    "    # We need to grab all the images and convert them into a singular video\n",
    "    if save_video_filepath is not None:\n",
    "      print(\"Generating video from frames...\")\n",
    "      # Grab all frames in our temp folder. Sort them humanly.\n",
    "      frames_raw = [img for img in os.listdir(video_dirname) if img.endswith(\".png\")]\n",
    "      frames = sort_nicely(frames_raw)\n",
    "      # We'll get the first frame and temporarily use it to determine the \n",
    "      frame = cv2.imread(os.path.join(video_dirname, frames[0]))\n",
    "      height, width, layers = frame.shape\n",
    "      # We'll also calcualte the FPS from our `dt`\n",
    "      video_fps = 1 / dt\n",
    "      # Initialize the video writer\n",
    "      video = cv2.VideoWriter(save_video_filepath, cv2.VideoWriter_fourcc(*\"MJPG\"), video_fps, (width, height))\n",
    "      # Iterate throgh our frames\n",
    "      try:\n",
    "        for i in range(len(frames)):\n",
    "          image = frames[i]\n",
    "          video.write(cv2.imread(os.path.join(video_dirname, image)))\n",
    "      except:\n",
    "        print(\"[ERROR] Something went wrong while processing the video. Ending early\")\n",
    "      # Release the video writer\n",
    "      video.release()\n",
    "      print(\"Video generated. Now deleting extraneous frames from temp folder\")\n",
    "      # Finally, delete the temp folder\n",
    "      if delete_video_frames and os.path.exists(video_dirname):\n",
    "        shutil.rmtree(video_dirname)\n",
    "      print(\"Video finished generating!\")\n",
    "\n",
    "    # close the plot\n",
    "    plt.close()\n",
    "\n",
    "  def GetInfo(self):\n",
    "    print(self.mne.info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07001f7",
   "metadata": {},
   "source": [
    "## Handling Participants' Data\n",
    "\n",
    "> **If you don't want to or can't collect BCI data on your own, included alongside this file is a collection of sample events and EEG data. You can use this sample to test the system. They're provided as `sample.zip`.**\n",
    ">\n",
    "> **The same dummy data is also available on [Google Drive](https://drive.google.com/file/d/1MrMkgPo894oH4kjxDW2QTkfSCh6VR81k/view?usp=drive_link) and [Github](https://github.com/SimpleDevs-AR-VR/CSGY9223-ARVR-EEGSS).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e6f72d",
   "metadata": {},
   "source": [
    "### Participant #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08885ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = {\n",
    "    \"eeg\":{\n",
    "        \"stand\": EEGInterpreter(\"P1: Stand\", \"./data/p1/stand.eeg.csv\", timestamp_col=\"unix_ts\", drop_channels=[\"AUX\"], min_freq=0, max_freq=100, verbose=False),\n",
    "        \"head\": EEGInterpreter(\"P1: Head\", \"./data/p1/head.eeg.csv\", timestamp_col=\"unix_ts\", drop_channels=[\"AUX\"], min_freq=0, max_freq=100, verbose=False),\n",
    "        \"eye\": EEGInterpreter(\"P1: Eye\", \"./data/p1/eye.eeg.csv\", timestamp_col=\"unix_ts\", drop_channels=[\"AUX\"], min_freq=0, max_freq=100, verbose=False),\n",
    "        \"move\": EEGInterpreter(\"P1: Move\", \"./data/p1/move.eeg.csv\", timestamp_col=\"unix_ts\", drop_channels=[\"AUX\"], min_freq=0, max_freq=100, verbose=False),\n",
    "        \"vr\": EEGInterpreter(\"P1: VR\", \"./data/p1/vr.eeg.csv\", timestamp_col=\"unix_ts\", drop_channels=[\"AUX\"], min_freq=0, max_freq=100, verbose=False)\n",
    "    },\n",
    "    \"events\": Events(\"./data/p1/vr.events.csv\")\n",
    "}\n",
    "for experiment in p1[\"eeg\"].values():\n",
    "  experiment.ReReference()\n",
    "print(p1[\"events\"].GetTrialInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2816f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1[\"eeg\"][\"vr\"].PlotPSDGantt(\n",
    "  title=\"Participant 1: Mean Peak Frequencies Across Time\",\n",
    "  start=11.85700011253357,\n",
    "  end=147.57500004768372,\n",
    "  fmin=0.5,\n",
    "  fmax=80,\n",
    "  figsize=(10,4),\n",
    "  frequencies=[\"delta\",\"theta\",\"alpha\",\"beta\",\"gamma\"],\n",
    "  colors=[\"black\",\"gray\",\"blue\",\"red\",\"orange\"],\n",
    "  peak_threshold=50,\n",
    "  representative_metric=\"mean\",\n",
    "  events=p1[\"events\"],\n",
    "  save_fig_filename=\"./data/p1/vr.gantt.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb265bdd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p1[\"eeg\"][\"vr\"].PlotPSDOverTime(\n",
    "  frequencies=[\"delta\",\"theta\",\"alpha\",\"beta\",\"gamma\"], pmax=200, \n",
    "  start=11.85700011253357, end=147.57500004768372,\n",
    "  dt=0.2, render_dt=0, \n",
    "  detect_peaks=True, peak_threshold=200,\n",
    "  events=p1[\"events\"],\n",
    "  save_video_filepath=\"./data/p1/vr.psd.avi\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831af696",
   "metadata": {},
   "source": [
    "### Participant #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06732e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "p2 = {\n",
    "    \"eeg\":{\n",
    "        \"stand\": EEGInterpreter(\"P2: Stand\", \"./data/p2/stand.eeg.csv\", timestamp_col=\"unix_ts\", drop_channels=[\"AUX\"], min_freq=0, max_freq=100, verbose=False),\n",
    "        \"head\": EEGInterpreter(\"P2: Head\", \"./data/p2/head.eeg.csv\", timestamp_col=\"unix_ts\", drop_channels=[\"AUX\"], min_freq=0, max_freq=100, verbose=False),\n",
    "        \"eye\": EEGInterpreter(\"P2: Eye\", \"./data/p2/eye.eeg.csv\", timestamp_col=\"unix_ts\", drop_channels=[\"AUX\"], min_freq=0, max_freq=100, verbose=False),\n",
    "        \"move\": EEGInterpreter(\"P2: Move\", \"./data/p2/move.eeg.csv\", timestamp_col=\"unix_ts\", drop_channels=[\"AUX\"], min_freq=0, max_freq=100, verbose=False),\n",
    "        \"vr\": EEGInterpreter(\"P2: VR\", \"./data/p2/vr.eeg.csv\", timestamp_col=\"unix_ts\", drop_channels=[\"AUX\"], min_freq=0, max_freq=100, verbose=False)\n",
    "    },\n",
    "    \"events\": Events(\"./data/p2/vr.events.csv\")\n",
    "}\n",
    "for experiment in p2[\"eeg\"].values():\n",
    "  experiment.ReReference()\n",
    "print(p2[\"events\"].GetTrialInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95127f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "p2[\"eeg\"][\"vr\"].PlotPSDGantt(\n",
    "  title=\"Participant 2: Mean Peak Frequencies Across Time\",\n",
    "  start=75.09300017356873,\n",
    "  end=244.54900002479553,\n",
    "  fmin=0.5,\n",
    "  fmax=80,\n",
    "  figsize=(10,4),\n",
    "  frequencies=[\"delta\",\"theta\",\"alpha\",\"beta\",\"gamma\"],\n",
    "  colors=[\"black\",\"gray\",\"blue\",\"red\",\"orange\"],\n",
    "  peak_threshold=50,\n",
    "  representative_metric=\"mean\",\n",
    "  events=p2[\"events\"],\n",
    "  save_fig_filename=\"./data/p2/vr.gantt.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500e5e6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p2[\"eeg\"][\"vr\"].PlotPSDOverTime(\n",
    "  frequencies=[\"delta\",\"theta\",\"alpha\",\"beta\",\"gamma\"],  pmax=200, \n",
    "  start=75.09300017356873, end=244.54900002479553, \n",
    "  dt=0.2, render_dt=0, \n",
    "  detect_peaks=True, peak_threshold=200,\n",
    "  events=p2[\"events\"],\n",
    "  save_video_filepath=\"./data/p2/vr.psd.avi\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51568a32",
   "metadata": {},
   "source": [
    "### Participant #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e6a217",
   "metadata": {},
   "outputs": [],
   "source": [
    "p3 = {\n",
    "    \"eeg\":{\n",
    "        \"stand\": EEGInterpreter(\"P3: Stand\", \"./data/p3/stand.eeg.csv\", timestamp_col=\"unix_ts\", drop_channels=[\"AUX\"], min_freq=0, max_freq=100, verbose=False),\n",
    "        \"head\": EEGInterpreter(\"P3: Head\", \"./data/p3/head.eeg.csv\", timestamp_col=\"unix_ts\", drop_channels=[\"AUX\"], min_freq=0, max_freq=100, verbose=False),\n",
    "        \"eye\": EEGInterpreter(\"P3: Eye\", \"./data/p3/eye.eeg.csv\", timestamp_col=\"unix_ts\", drop_channels=[\"AUX\"], min_freq=0, max_freq=100, verbose=False),\n",
    "        \"move\": EEGInterpreter(\"P3: Move\", \"./data/p3/move.eeg.csv\", timestamp_col=\"unix_ts\", drop_channels=[\"AUX\"], min_freq=0, max_freq=100, verbose=False),\n",
    "        \"vr\": EEGInterpreter(\"P3: VR\", \"./data/p3/vr.eeg.csv\", timestamp_col=\"unix_ts\", drop_channels=[\"AUX\"], min_freq=0, max_freq=100, verbose=False)\n",
    "    },\n",
    "    \"events\": Events(\"./data/p3/vr.events.csv\")\n",
    "}\n",
    "for experiment in p3[\"eeg\"].values():\n",
    "  experiment.ReReference()\n",
    "print(p3[\"events\"].GetTrialInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2beb3025",
   "metadata": {},
   "outputs": [],
   "source": [
    "p3[\"eeg\"][\"vr\"].PlotPSDGantt(\n",
    "  title=\"Participant 3: Mean Peak Frequencies Across Time\",\n",
    "  start=15.316999912261963,\n",
    "  end=213.3329999446869,\n",
    "  fmin=0.5,\n",
    "  fmax=80,\n",
    "  figsize=(10,4),\n",
    "  frequencies=[\"delta\",\"theta\",\"alpha\",\"beta\",\"gamma\"],\n",
    "  colors=[\"black\",\"gray\",\"blue\",\"red\",\"orange\"],\n",
    "  peak_threshold=50,\n",
    "  representative_metric=\"mean\",\n",
    "  events=p3[\"events\"],\n",
    "  save_fig_filename=\"./data/p3/vr.gantt.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02780089",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p3[\"eeg\"][\"vr\"].PlotPSDOverTime(\n",
    "  frequencies=[\"delta\",\"theta\",\"alpha\",\"beta\",\"gamma\"],  pmax=200, \n",
    "  start=15.316999912261963, end= 102.43899989128113, \n",
    "  dt=0.2, render_dt=0, \n",
    "  detect_peaks=True, peak_threshold=200,\n",
    "  events=p3[\"events\"],\n",
    "  save_video_filepath=\"./data/p3/vr.psd.avi\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7d24ca",
   "metadata": {},
   "source": [
    "### Participant #4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfe17d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "p4 = {\n",
    "    \"eeg\":{\n",
    "        \"stand\": EEGInterpreter(\"P4: Stand\", \"./data/p4/stand.eeg.csv\", timestamp_col=\"unix_ts\", drop_channels=[\"AUX\"], min_freq=0, max_freq=100, verbose=False),\n",
    "        \"head\": EEGInterpreter(\"P4: Head\", \"./data/p4/head.eeg.csv\", timestamp_col=\"unix_ts\", drop_channels=[\"AUX\"], min_freq=0, max_freq=100, verbose=False),\n",
    "        \"eye\": EEGInterpreter(\"P4: Eye\", \"./data/p4/eye.eeg.csv\", timestamp_col=\"unix_ts\", drop_channels=[\"AUX\"], min_freq=0, max_freq=100, verbose=False),\n",
    "        \"move\": EEGInterpreter(\"P4: Move\", \"./data/p4/move.eeg.csv\", timestamp_col=\"unix_ts\", drop_channels=[\"AUX\"], min_freq=0, max_freq=100, verbose=False),\n",
    "        \"vr\": EEGInterpreter(\"P4: VR\", \"./data/p4/vr.eeg.csv\", timestamp_col=\"unix_ts\", drop_channels=[\"AUX\"], min_freq=0, max_freq=100, verbose=False)\n",
    "    },\n",
    "    \"events\": Events(\"./data/p4/vr.events.csv\")\n",
    "}\n",
    "for experiment in p4[\"eeg\"].values():\n",
    "  experiment.ReReference()\n",
    "print(p4[\"events\"].GetTrialInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef62d8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "p4[\"eeg\"][\"vr\"].PlotPSDGantt(\n",
    "  title=\"Participant 4: Mean Peak Frequencies Across Time\",\n",
    "  start=41.401999950408936,\n",
    "  end=213.27699995040894,\n",
    "  fmin=0.5,\n",
    "  fmax=80,\n",
    "  figsize=(10,4),\n",
    "  frequencies=[\"delta\",\"theta\",\"alpha\",\"beta\",\"gamma\"],\n",
    "  colors=[\"black\",\"gray\",\"blue\",\"red\",\"orange\"],\n",
    "  peak_threshold=50,\n",
    "  representative_metric=\"mean\",\n",
    "  events=p4[\"events\"],\n",
    "  save_fig_filename=\"./data/p4/vr.gantt.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058642ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p4[\"eeg\"][\"vr\"].PlotPSDOverTime(\n",
    "  frequencies=[\"delta\",\"theta\",\"alpha\",\"beta\",\"gamma\"],  pmax=200, \n",
    "  start=41.401999950408936, end=159.81199979782104, \n",
    "  dt=0.2, render_dt=0, \n",
    "  detect_peaks=True, peak_threshold=200,\n",
    "  events=p4[\"events\"],\n",
    "  save_video_filepath=\"./data/p4/vr.psd.avi\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71a823a",
   "metadata": {},
   "source": [
    "### Participant #5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28004241",
   "metadata": {},
   "outputs": [],
   "source": [
    "p5 = {\n",
    "    \"eeg\":{\n",
    "        \"stand\": EEGInterpreter(\"P5: Stand\", \"./data/p5/stand.eeg.csv\", timestamp_col=\"unix_ts\", drop_channels=[\"AUX\"], min_freq=0, max_freq=100, verbose=False),\n",
    "        \"head\": EEGInterpreter(\"P5: Head\", \"./data/p5/head.eeg.csv\", timestamp_col=\"unix_ts\", drop_channels=[\"AUX\"], min_freq=0, max_freq=100, verbose=False),\n",
    "        \"eye\": EEGInterpreter(\"P5: Eye\", \"./data/p5/eye.eeg.csv\", timestamp_col=\"unix_ts\", drop_channels=[\"AUX\"], min_freq=0, max_freq=100, verbose=False),\n",
    "        \"move\": EEGInterpreter(\"P5: Move\", \"./data/p5/move.eeg.csv\", timestamp_col=\"unix_ts\", drop_channels=[\"AUX\"], min_freq=0, max_freq=100, verbose=False),\n",
    "        \"vr\": EEGInterpreter(\"P5: VR\", \"./data/p5/vr.eeg.csv\", timestamp_col=\"unix_ts\", drop_channels=[\"AUX\"], min_freq=0, max_freq=100, verbose=False)\n",
    "    },\n",
    "    \"events\": Events(\"./data/p5/vr.events.csv\")\n",
    "}\n",
    "for experiment in p5[\"eeg\"].values():\n",
    "  experiment.ReReference()\n",
    "print(p5[\"events\"].GetTrialInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaec4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "p5[\"eeg\"][\"vr\"].PlotPSDGantt(\n",
    "  title=\"Participant 5: Mean Peak Frequencies Across Time\",\n",
    "  start=41.58400011062622,\n",
    "  end=197.54800009727478,\n",
    "  fmin=0.5,\n",
    "  fmax=80,\n",
    "  figsize=(10,4),\n",
    "  frequencies=[\"delta\",\"theta\",\"alpha\",\"beta\",\"gamma\"],\n",
    "  colors=[\"black\",\"gray\",\"blue\",\"red\",\"orange\"],\n",
    "  peak_threshold=50,\n",
    "  representative_metric=\"mean\",\n",
    "  events=p5[\"events\"],\n",
    "  save_fig_filename=\"./data/p5/vr.gantt.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbf9a9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p5[\"eeg\"][\"vr\"].PlotPSDOverTime(\n",
    "  frequencies=[\"delta\",\"theta\",\"alpha\",\"beta\",\"gamma\"],  pmax=200, \n",
    "  start=41.58400011062622, end=197.54800009727478, \n",
    "  dt=0.2, render_dt=0, \n",
    "  detect_peaks=True, peak_threshold=200,\n",
    "  events=p5[\"events\"],\n",
    "  save_video_filepath=\"./data/p5/vr.psd.avi\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1135e205",
   "metadata": {},
   "outputs": [],
   "source": [
    "p6 = {\n",
    "    \"eeg\":{\n",
    "        \"test\": EEGInterpreter(\"P6: Test\", \"./logs/EEG Stream_2/eeg.csv\", timestamp_col=\"unix_ts\", drop_channels=[\"AUX\"], min_freq=0, max_freq=100, verbose=False)\n",
    "    }\n",
    "}\n",
    "for experiment in p6[\"eeg\"].values():\n",
    "  experiment.ReReference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a510aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "p6[\"eeg\"][\"test\"].PlotPSDGantt(\n",
    "  title=\"Participant 6: Mean Peak Frequencies Across Time\",\n",
    "  fmin=0.5,\n",
    "  fmax=80,\n",
    "  figsize=(10,4),\n",
    "  frequencies=[\"delta\",\"theta\",\"alpha\",\"beta\",\"gamma\"],\n",
    "  colors=[\"black\",\"gray\",\"blue\",\"red\",\"orange\"],\n",
    "  peak_threshold=50,\n",
    "  representative_metric=\"mean\",\n",
    "    save_fig_filename=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b7bc8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
